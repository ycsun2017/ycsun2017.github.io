<!DOCTYPE html>
<html lang="en">

  <head>

  </head>
<head>
  <!-- Title -->
  <title>Vulnerability-Aware Poisoning Mechanism for Online RL with Unknown Dynamics</title>

  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Prediction-Guided Multi-Objective Reinforcement Learning for Continuous Robot Control">
  <meta name="keywords" content="Prediction-Guided, Multi-Objective Reinforcement Learning, PGMORL">

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <!-- https://fontawesome.com/cheatsheet -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-124898353-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-124898353-1');
  </script>


</head>


<body>
  <!-- <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"> -->
  <nav class="navbar navbar-expand-md fixed-top navbar-dark" style="background-color: #1631c7;">
    <a class="navbar-brand" href="#">VA2C-P: Poisoning Deep Reinforcement Learning </a>

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarToggle">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link" href="#">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Abstract">Abstract</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Poster">Poster</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Cite">Cite</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="container" style="padding-top: 80px; font-size: 20px">
    <div align="center">
      <h2 class="text-center" align="center">
        Vulnerability-Aware Poisoning Mechanism for Online RL with Unknown Dynamics
      </h2><br>
      <h6>
      <!--  <a href="https://people.csail.mit.edu/jiex">Jie Xu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;  -->
      Yanchao Sun</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      Da Huo</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      Furong Huang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      </h6>
      <small><sup>1</sup> University of Maryland College Park &nbsp;&nbsp;&nbsp;&nbsp; <sup>2</sup>  Shanghai Jiao Tong University
</small>
        <i><font color="black"> </font></i><br>
        <a target="_blank" href="https://openreview.net/forum?id=9r30XCjf5Dt">[Paper]</a>&nbsp;
        <a target="_blank" href="https://github.com/umd-huang-lab/poison-rl">[Code]</a>&nbsp;
    </div>
  </div><br>


  <!-- Abstract -->
  <div class="container">
    <h4 id="Abstract" style="padding-top: 70px; margin-top: -80px; ">Abstract</h4><hr>
    <div style="text-align: justify">
      Poisoning attacks on Reinforcement Learning (RL) systems could take advantage of RL algorithm’s vulnerabilities and cause failure of the learning. However, prior works on poisoning RL usually either unrealistically assume the attacker knows the underlying Markov Decision Process (MDP), or directly apply the poisoning methods in supervised learning to RL. In this work, we build a generic poisoning framework for online RL via a comprehensive investigation of heterogeneous poisoning models in RL. Without any prior knowledge of the MDP, we propose a strategic poisoning algorithm called Vulnerability-Aware Adversarial Critic Poison (VA2C-P), which works for on-policy deep RL agents, closing the gap that no poisoning method exists for policy-based RL agents. VA2C-P uses a novel metric, stability radius in RL, that measures the vulnerability of RL algorithms. Experiments on multiple deep RL agents and multiple environments show that our poisoning algorithm successfully prevents agents from learning a good policy or teaches the agents to converge to a target policy, with a limited attacking budget.
    </div>
  </div><br><br>


  <!-- Method: Breifly Describe our algorithm -->
  <!-- <div class="container">
    <h4 id="Method" style="padding-top: 70px; margin-top: -80px; ">Method</h4><hr>
    <b>Previous Appoach</b><br>
    <ul>
      Prior works in evasion RL generally falls into two categories: <b>Heuristic Attack</b> and <b>RL-based Attack</b><br>
      <li><b>Heuristic Attack</b>: at every step set an attacking objective by heuristic. For example, use fast gradient sign method (<b>FGSM</b>) to find
      the perturbation that could cause the agent to select the action corresponding to the minimum current Q value (<b>MaxWorst</b>). Or at every step, use FGSM to stop the agents
      from choosing the best action (<b>MinBest</b>). Or at every step, find a perturbation that cause the action distribution to have the largest KL divergence from the original
      distribution (<b>MaxDiff</b>). Heuristic Attack is often fast to compute and works decently well in practice, but it has no optimality guarantee.</li>
      <li><b>RL-based attack</b>: <a href=https://arxiv.org/pdf/2101.08452.pdf>Zhang et al.</a> establish the theory that learning optimal attacker in RL corresponds to learning
      an optimal policy in an adversary-induced MDP. Based on this insight, they use an end-to-end RL formulation to learn the optimal state adversary policy. However, because
      both the state and action space of the attacker's policy have the same size as the state space of the original environment, it becomes intractable when the original environments
      has a large state space, such as image space in Atari Games.  </li>
  </ul>
    <b> Our Approach (PA-AD) </b>:
      Instead of learning an end-to-end adversary policy that maps observations directly into observation perturbation, we decouple the whole attacking process into two
      simpler components: policy perturbation and state perturbation, solved by a <b>“director”</b> and an <b>“actor”</b> respectively. The director learns the
      optimal policy perturbing direction with RL methods, while the actor crafts adversarial states at every step such that the victim policy is perturbed towards the given direction.
      In this way, we obtain a both optimal and efficient attacking algorithm which is extremely powerful on environments with large observation space.
      <img class="img-responsive img-rounded" src="demo.PNG" style="width:100%; height:100%" alt="">

    <div style="text-align: justify">

    </div>
  </div><br><br> -->

  <div class="container">
    <h4 id="Poster" style="padding-top: 70px; margin-top: -80px; ">Poster</h4><hr>
    <center><img class="img-responsive img-rounded" src="./poster.png" style="width:100%; height:100%" alt=""></center>
  </div><br><br>

  <!--
  <div class="container">
    <h4 id="Citation" style="padding-top: 70px; margin-top: -80px;">Citation</h4><hr>
    <div class="row">
      <div class="col-md-12">
        <b><a href="https://arxiv.org/pdf/2003.08938.pdf">Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations</a></b><br/>
          Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, Cho-Jui Hsieh1 <br/>
        <i>Neural Information Processing Systems (Neurips 2020)</i><br/>
        <b><a href="https://arxiv.org/abs/2101.08452">Robust Reinforcement Learning on State Observations with Learned Optimal Adversary</a></b><br/>
        Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh1 <br/>
        <i> International Conference on Learning Representations (ICLR 2021)</i><br/>
        <b><a href="https://arxiv.org/abs/2101.08452">Robust Reinforcement Learning on State Observations with Learned Optimal Adversary</a></b><br/>
        Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh1 <br/>
        <i> International Conference on Learning Representations (ICLR 2021)</i><br/>
      </div>
    </div>
  </div> <br> -->

  <div class="container">
    <h4 id="Cite" style="padding-top: 70px; margin-top: -80px;">Cite This Paper</h4><hr>
    <div class="row">
      <div class="col-md-12">
        @inproceedings{<br>
          &nbsp;&nbsp;&nbsp;&nbsp;sun2021vulnerabilityaware,<br>
          &nbsp;&nbsp;&nbsp;&nbsp;title={Vulnerability-Aware Poisoning Mechanism for Online {\{}RL{\}} with Unknown Dynamics},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;author={Yanchao Sun and Da Huo and Furong Huang},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;booktitle={International Conference on Learning Representations},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;year={2021},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;url={https://openreview.net/forum?id=9r30XCjf5Dt}<br>
          }<br>
      </div>
    </div>
  </div> <br>
  <!-- Footer -->
  <div class="container col-md-9">
    <hr>
    <center>
      <footer>
        <p>© University of Maryland College Park, 2021</p>
      </footer>
    </center>
  </div>


  <!-- Bootstrap core JavaScript -->
  <!-- Placed at the end of the document so the pages load faster -->
  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>

</body>

</html>
