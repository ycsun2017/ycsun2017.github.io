<!DOCTYPE html>
<html lang="en">

  <head>

  </head>
<head>
  <!-- Title -->
  <title>Transfer RL across Observation Feature Spaces via Model-Based Regularization</title>

  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Prediction-Guided Multi-Objective Reinforcement Learning for Continuous Robot Control">
  <meta name="keywords" content="Prediction-Guided, Multi-Objective Reinforcement Learning, PGMORL">

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <!-- https://fontawesome.com/cheatsheet -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-124898353-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-124898353-1');
  </script>


</head>


<body>
  <!-- <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"> -->
  <nav class="navbar navbar-expand-md fixed-top navbar-dark" style="background-color: #1631c7;">
    <a class="navbar-brand" href="#">Transfer RL across Observation Feature Spaces via Model-Based Regularization </a>

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarToggle">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link" href="#">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Abstract">Abstract</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Poster">Poster</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Cite">Cite</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="container" style="padding-top: 80px; font-size: 20px">
    <div align="center">
      <h2 class="text-center" align="center">
        Transfer RL across Observation Feature Spaces <br> via Model-Based Regularization
      </h2><br>
      <h6>
      <!--  <a href="https://people.csail.mit.edu/jiex">Jie Xu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;  -->
      Yanchao Sun</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      Ruijie Zheng</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      Xiyao Wang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      Andrew Cohen</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      Furong Huang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      </h6>
      <small><sup>1</sup> University of Maryland College Park &nbsp;&nbsp;&nbsp;&nbsp; <sup>2</sup>  Unity Technologies
</small>
        <i><font color="black"> </font></i><br>
        <a target="_blank" href="https://openreview.net/pdf?id=7KdAoOsI81C">[Paper]</a>&nbsp;
        <a target="_blank" href="https://github.com/umd-huang-lab/transfer_across_obs">[Code]</a>&nbsp;
    </div>
  </div><br>


  <!-- Abstract -->
  <div class="container">
    <h4 id="Abstract" style="padding-top: 70px; margin-top: -80px; ">Abstract</h4><hr>
    <div style="text-align: justify">
      In many reinforcement learning (RL) applications, the observation space is specified by human developers and restricted by physical realizations, and may thus be subject to dramatic changes over time (e.g. increased number of observable features). However, when the observation space changes, the previous policy will likely fail due to the mismatch of input features, and another policy must be trained from scratch, which is inefficient in terms of computation and sample complexity. Following theoretical insights, we propose a novel algorithm which extracts the latent-space dynamics in the source task, and transfers the dynamics model to the target task to use as a model-based regularizer. Our algorithm works for drastic changes of observation space (e.g. from vector-based observation to image-based observation), without any inter-task mapping or any prior knowledge of the target task. Empirical results show that our algorithm significantly improves the efficiency and stability of learning in the target task.
    </div>
  </div><br><br>


  <!-- Method: Breifly Describe our algorithm -->
  <!-- <div class="container">
    <h4 id="Method" style="padding-top: 70px; margin-top: -80px; ">Method</h4><hr>
    <b>Previous Appoach</b><br>
    <ul>
      Prior works in evasion RL generally falls into two categories: <b>Heuristic Attack</b> and <b>RL-based Attack</b><br>
      <li><b>Heuristic Attack</b>: at every step set an attacking objective by heuristic. For example, use fast gradient sign method (<b>FGSM</b>) to find
      the perturbation that could cause the agent to select the action corresponding to the minimum current Q value (<b>MaxWorst</b>). Or at every step, use FGSM to stop the agents
      from choosing the best action (<b>MinBest</b>). Or at every step, find a perturbation that cause the action distribution to have the largest KL divergence from the original
      distribution (<b>MaxDiff</b>). Heuristic Attack is often fast to compute and works decently well in practice, but it has no optimality guarantee.</li>
      <li><b>RL-based attack</b>: <a href=https://arxiv.org/pdf/2101.08452.pdf>Zhang et al.</a> establish the theory that learning optimal attacker in RL corresponds to learning
      an optimal policy in an adversary-induced MDP. Based on this insight, they use an end-to-end RL formulation to learn the optimal state adversary policy. However, because
      both the state and action space of the attacker's policy have the same size as the state space of the original environment, it becomes intractable when the original environments
      has a large state space, such as image space in Atari Games.  </li>
  </ul>
    <b> Our Approach (PA-AD) </b>:
      Instead of learning an end-to-end adversary policy that maps observations directly into observation perturbation, we decouple the whole attacking process into two
      simpler components: policy perturbation and state perturbation, solved by a <b>“director”</b> and an <b>“actor”</b> respectively. The director learns the
      optimal policy perturbing direction with RL methods, while the actor crafts adversarial states at every step such that the victim policy is perturbed towards the given direction.
      In this way, we obtain a both optimal and efficient attacking algorithm which is extremely powerful on environments with large observation space.
      <img class="img-responsive img-rounded" src="demo.PNG" style="width:100%; height:100%" alt="">

    <div style="text-align: justify">

    </div>
  </div><br><br> -->

  <div class="container">
    <h4 id="Poster" style="padding-top: 70px; margin-top: -80px; ">Poster</h4><hr>
    <center><img class="img-responsive img-rounded" src="./poster.png" style="width:100%; height:100%" alt=""></center>
  </div><br><br>

  <!--
  <div class="container">
    <h4 id="Citation" style="padding-top: 70px; margin-top: -80px;">Citation</h4><hr>
    <div class="row">
      <div class="col-md-12">
        <b><a href="https://arxiv.org/pdf/2003.08938.pdf">Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations</a></b><br/>
          Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, Cho-Jui Hsieh1 <br/>
        <i>Neural Information Processing Systems (Neurips 2020)</i><br/>
        <b><a href="https://arxiv.org/abs/2101.08452">Robust Reinforcement Learning on State Observations with Learned Optimal Adversary</a></b><br/>
        Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh1 <br/>
        <i> International Conference on Learning Representations (ICLR 2021)</i><br/>
        <b><a href="https://arxiv.org/abs/2101.08452">Robust Reinforcement Learning on State Observations with Learned Optimal Adversary</a></b><br/>
        Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh1 <br/>
        <i> International Conference on Learning Representations (ICLR 2021)</i><br/>
      </div>
    </div>
  </div> <br> -->

  <div class="container">
    <h4 id="Cite" style="padding-top: 70px; margin-top: -80px;">Cite This Paper</h4><hr>
    <div class="row">
      <div class="col-md-12">
        @inproceedings{<br>
          &nbsp;&nbsp;&nbsp;&nbsp;sun2022transfer,<br>
          &nbsp;&nbsp;&nbsp;&nbsp;title={Transfer {RL} across Observation Feature Spaces via Model-Based Regularization},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;author={Yanchao Sun and Ruijie Zheng and Xiyao Wang and Andrew E Cohen and Furong Huang},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;booktitle={International Conference on Learning Representations},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;year={2022},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;url={https://openreview.net/forum?id=7KdAoOsI81C}<br>
          }<br>
      </div>
    </div>
  </div> <br>

  <!-- Footer -->
  <div class="container col-md-9">
    <hr>
    <center>
      <footer>
        <p>© University of Maryland College Park, 2022</p>
      </footer>
    </center>
  </div>


  <!-- Bootstrap core JavaScript -->
  <!-- Placed at the end of the document so the pages load faster -->
  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>

</body>

</html>
