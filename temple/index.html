<!DOCTYPE html>
<html lang="en">

  <head>

  </head>
<head>
  <!-- Title -->
  <title>TempLe: Learning Template of Transitions for Sample Efficient Multi-task RL</title>

  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Prediction-Guided Multi-Objective Reinforcement Learning for Continuous Robot Control">
  <meta name="keywords" content="Prediction-Guided, Multi-Objective Reinforcement Learning, PGMORL">

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <!-- https://fontawesome.com/cheatsheet -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-124898353-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-124898353-1');
  </script>


</head>


<body>
  <!-- <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"> -->
  <nav class="navbar navbar-expand-md fixed-top navbar-dark" style="background-color: #1631c7;">
    <a class="navbar-brand" href="#">TempLe: Learning Template of Transitions </a>

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarToggle">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link" href="#">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Abstract">Abstract</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Poster">Poster</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Cite">Cite</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="container" style="padding-top: 80px; font-size: 20px">
    <div align="center">
      <h2 class="text-center" align="center">
        TempLe: Learning Template of Transitions for Sample Efficient Multi-task RL
      </h2><br>
      <h6>
      <!--  <a href="https://people.csail.mit.edu/jiex">Jie Xu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;  -->
      Yanchao Sun</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      Xiangyu Yin</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      Furong Huang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      </h6>
      <small><sup>1</sup> University of Maryland College Park &nbsp;&nbsp;&nbsp;&nbsp; <sup>2</sup>  Beijing University of Posts and Telecommunications
</small>
        <i><font color="black"> </font></i><br>
        <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/17174">[Paper]</a>&nbsp;
        <a target="_blank" href="https://github.com/umd-huang-lab/template-reinforcement-learning">[Code]</a>&nbsp;
    </div>
  </div><br>


  <!-- Abstract -->
  <div class="container">
    <h4 id="Abstract" style="padding-top: 70px; margin-top: -80px; ">Abstract</h4><hr>
    <div style="text-align: justify">
      Transferring knowledge among various environments is important to efficiently learn multiple tasks online. Most existing methods directly use the previously learned models or previously learned optimal policies to learn new tasks. However, these methods may be inefficient when the underlying models or optimal policies are substantially different across tasks. In this paper, we propose Template Learning (TempLe), the first PAC-MDP method for multi-task reinforcement learning that could be applied to tasks with varying state/action space. TempLe generates transition dynamics templates, abstractions of the transition dynamics across tasks, to gain sample efficiency by extracting similarities between tasks even when their underlying models or optimal policies have limited commonalities. We present two algorithms for an "online" and a "finite-model" setting respectively. We prove that our proposed TempLe algorithms achieve much lower sample complexity than single-task learners or state-of-the-art multi-task methods. We show via systematically designed experiments that our TempLe method universally outperforms the state-of-the-art multi-task methods (PAC-MDP or not) in various settings and regimes.  
    </div>
  </div><br><br>


  <!-- Method: Breifly Describe our algorithm -->
  <!-- <div class="container">
    <h4 id="Method" style="padding-top: 70px; margin-top: -80px; ">Method</h4><hr>
    <b>Previous Appoach</b><br>
    <ul>
      Prior works in evasion RL generally falls into two categories: <b>Heuristic Attack</b> and <b>RL-based Attack</b><br>
      <li><b>Heuristic Attack</b>: at every step set an attacking objective by heuristic. For example, use fast gradient sign method (<b>FGSM</b>) to find
      the perturbation that could cause the agent to select the action corresponding to the minimum current Q value (<b>MaxWorst</b>). Or at every step, use FGSM to stop the agents
      from choosing the best action (<b>MinBest</b>). Or at every step, find a perturbation that cause the action distribution to have the largest KL divergence from the original
      distribution (<b>MaxDiff</b>). Heuristic Attack is often fast to compute and works decently well in practice, but it has no optimality guarantee.</li>
      <li><b>RL-based attack</b>: <a href=https://arxiv.org/pdf/2101.08452.pdf>Zhang et al.</a> establish the theory that learning optimal attacker in RL corresponds to learning
      an optimal policy in an adversary-induced MDP. Based on this insight, they use an end-to-end RL formulation to learn the optimal state adversary policy. However, because
      both the state and action space of the attacker's policy have the same size as the state space of the original environment, it becomes intractable when the original environments
      has a large state space, such as image space in Atari Games.  </li>
  </ul>
    <b> Our Approach (PA-AD) </b>:
      Instead of learning an end-to-end adversary policy that maps observations directly into observation perturbation, we decouple the whole attacking process into two
      simpler components: policy perturbation and state perturbation, solved by a <b>“director”</b> and an <b>“actor”</b> respectively. The director learns the
      optimal policy perturbing direction with RL methods, while the actor crafts adversarial states at every step such that the victim policy is perturbed towards the given direction.
      In this way, we obtain a both optimal and efficient attacking algorithm which is extremely powerful on environments with large observation space.
      <img class="img-responsive img-rounded" src="demo.PNG" style="width:100%; height:100%" alt="">

    <div style="text-align: justify">

    </div>
  </div><br><br> -->

  <div class="container">
    <h4 id="Poster" style="padding-top: 70px; margin-top: -80px; ">Poster</h4><hr>
    <center><img class="img-responsive img-rounded" src="./poster.png" style="width:100%; height:100%" alt=""></center>
  </div><br><br>

  <!--
  <div class="container">
    <h4 id="Citation" style="padding-top: 70px; margin-top: -80px;">Citation</h4><hr>
    <div class="row">
      <div class="col-md-12">
        <b><a href="https://arxiv.org/pdf/2003.08938.pdf">Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations</a></b><br/>
          Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, Cho-Jui Hsieh1 <br/>
        <i>Neural Information Processing Systems (Neurips 2020)</i><br/>
        <b><a href="https://arxiv.org/abs/2101.08452">Robust Reinforcement Learning on State Observations with Learned Optimal Adversary</a></b><br/>
        Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh1 <br/>
        <i> International Conference on Learning Representations (ICLR 2021)</i><br/>
        <b><a href="https://arxiv.org/abs/2101.08452">Robust Reinforcement Learning on State Observations with Learned Optimal Adversary</a></b><br/>
        Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh1 <br/>
        <i> International Conference on Learning Representations (ICLR 2021)</i><br/>
      </div>
    </div>
  </div> <br> -->

  <div class="container">
    <h4 id="Cite" style="padding-top: 70px; margin-top: -80px;">Cite This Paper</h4><hr>
    <div class="row">
      <div class="col-md-12">
        Sun, Y., Yin, X. and Huang, F. 2021. TempLe: Learning Template of Transitions for Sample Efficient Multi-task RL. Proceedings of the AAAI Conference on Artificial Intelligence. 35, 11 (May 2021), 9765-9773.
        <br>
        <a href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/download/bibtex?submissionId=17174&publicationId=15490">Bibtex</a>
      </div>
    </div>
  </div> <br>
  <!-- Footer -->
  <div class="container col-md-9">
    <hr>
    <center>
      <footer>
        <p>© University of Maryland College Park, 2021</p>
      </footer>
    </center>
  </div>


  <!-- Bootstrap core JavaScript -->
  <!-- Placed at the end of the document so the pages load faster -->
  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>

</body>

</html>
