<!DOCTYPE html>
<html lang="en">

  <head>

  </head>
<head>
  <!-- Title -->
  <title>Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL</title>

  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Prediction-Guided Multi-Objective Reinforcement Learning for Continuous Robot Control">
  <meta name="keywords" content="Prediction-Guided, Multi-Objective Reinforcement Learning, PGMORL">

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <!-- https://fontawesome.com/cheatsheet -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-124898353-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-124898353-1');
  </script>


</head>


<body>
  <!-- <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"> -->
  <nav class="navbar navbar-expand-md fixed-top navbar-dark" style="background-color: #1631c7;">
    <a class="navbar-brand" href="#">PA-AD: Optimal and Efficient Evasion RL </a>

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarToggle">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link" href="#">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Abstract">Abstract</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Method">Method</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Results">Results</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#Cite">Cite</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="container" style="padding-top: 80px; font-size: 20px">
    <div align="center">
      <h2 class="text-center" align="center">
        Who Is the Strongest Enemy? <br> Towards Optimal and Efficient Evasion Attacks in Deep RL
      </h2><br>
      <h6>
      <!--  <a href="https://people.csail.mit.edu/jiex">Jie Xu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;  -->
      Yanchao Sun</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      Ruijie Zheng</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      Yongyuan Liang</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      Furong Huang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      </h6>
      <small><sup>1</sup> University of Maryland College Park &nbsp;&nbsp;&nbsp;&nbsp; <sup>2</sup>  Sun Yat-sen University
</small>
        <i><font color="black"> </font></i><br>
        <a target="_blank" href="https://openreview.net/pdf?id=JM2kFbJvvI">[Paper]</a>&nbsp;
        <a target="_blank" href="https://github.com/umd-huang-lab/paad_adv_rl">[Code]</a>&nbsp;
    </div>
  </div><br>

  <!--<div class="container" style="padding-top: 10px; font-size: 20px">
    <div align="center">
      <div class="center">
        <img class="img-responsive img-rounded" src="optimal_halfcheetah.gif" style="width:15%; height:15%" alt="">&nbsp;&nbsp;
        <img class="img-responsive img-rounded" src="attack_halfcheetah.gif" style="width:15%; height:15%" alt="">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <img class="img-responsive img-rounded" src="optimal_walker.gif" style="width:15%; height:15%" alt="">&nbsp;&nbsp;
        <img class="img-responsive img-rounded" src="attack_walker.gif" style="width:15%; height:15%" alt="">
        <figcaption><h1 style="font-size:1vw">Agent with & without attacker on two MuJoco Environments: HalfCheetah and Walker2d  </figcaption><br>
        <img class="img-responsive img-rounded" src="PongNoFrameskip-v4_None.gif" style="width:14%; height:15%" alt="">&nbsp;&nbsp;&nbsp;&nbsp;
        <img class="img-responsive img-rounded" src="PongNoFrameskip-v4_minbest.gif" style="width:14%; height:15%" alt="">&nbsp;&nbsp;&nbsp;&nbsp;
        <img class="img-responsive img-rounded" src="PongNoFrameskip-v4_paad.gif" style="width:14%; height:15%" alt="">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <img class="img-responsive img-rounded" src="BoxingNoFrameskip-v4_None.gif" style="width:14%; height:14%" alt="">&nbsp;&nbsp;&nbsp;&nbsp;
        <img class="img-responsive img-rounded" src="BoxingNoFrameskip-v4_minbest.gif" style="width:14%; height:14%" alt="">&nbsp;&nbsp;&nbsp;&nbsp;
        <img class="img-responsive img-rounded" src="BoxingNoFrameskip-v4_paad.gif" style="width:14%; height:14%" alt="">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <figcaption> <font size="-0.5"> <p align="left">&nbsp;&nbsp;&nbsp;&nbsp; <b>Pong</b>: no attack &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;
                                              <b>Pong</b>: strongest previous attack  &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; <b>Pong</b>: PA-AD attack </p>  </font></figcaption><br>
               <!--<figcaption> <font size="-0.5"> <p align="left">&nbsp;&nbsp;&nbsp;&nbsp; no attack &nbsp;&nbsp;
                                           previous attack  &nbsp;&nbsp;&nbsp;  </font></figcaption><br>-->
        <center>
        <table>
          <thead>
          <tr>
          <th align="center"><img src="hopper.gif" alt="Pong-attack-natural.gif" style="width:100%"></a></th>
          <th align="center"><img src="hopper_sa.gif" alt="Pong-attack-natural.gif" style="width:100%;"></a></th>
          <th align="center"><img src="hopper_pa.gif" alt="RoadRunner-attack-natural.gif" style="width:100%"></a></th>
          <th align="center"><img src="walker.gif" alt="Pong-attack-natural.gif" style="width:100%"></a></th>
          <th align="center"><img src="walker_sa.gif" alt="Pong-attack-natural.gif" style="width:100%"></a></th>
          <th align="center"><img src="walker_pa.gif" alt="Pong-attack-natural.gif" style="width:100%"></a></th>
          </tr>
        </thead>
          <tbody>
          <tr>
          <td align="center"><strong>Hopper</strong>, <em>PPO</em> <br> clean reward: <strong>3104</strong> <br></td>
          <td align="center"><strong>Hopper</strong>, <em>PPO</em> <br> reward under strongest <br> previous attack: <strong>636</strong> <br></td>
          <td align="center"><strong>Hopper</strong>, <em>PPO</em> <br> reward under our <br> PA-AD attack: <strong>171</strong> <br></td>
          <td align="center"><strong>Walker</strong>, <em>PPO</em> <br> clean reward: <strong>4630</strong> <br></td>
          <td align="center"><strong>Walker</strong>, <em>PPO</em> <br> reward under strongest <br> previous attack: <strong>1048</strong> <br></td>
          <td align="center"><strong>Walker</strong>, <em>PPO</em> <br> reward under our <br> PA-AD attack: <strong>813</strong> <br></td>
          </tr>
        <tr> </tr><tr> </tr>
        <tr>
        <td align="center"><img src="PongNoFrameskip-v4_None.gif" alt="Pong-attack-natural.gif" style="width:60%;"></a></th>
        <td align="center"><img src="PongNoFrameskip-v4_minbest.gif" alt="Pong-attack-natural.gif" style="width:60%;"></a></th>
        <td align="center"><img src="PongNoFrameskip-v4_paad.gif" alt="RoadRunner-attack-natural.gif" style="width:60%;"></a></th>
        <td align="center"><img src="BoxingNoFrameskip-v4_None.gif" alt="Pong-attack-natural.gif" style="width:60%;"></a></th>
        <td align="center"><img src="BoxingNoFrameskip-v4_minbest.gif" alt="Pong-attack-natural.gif" style="width:60%;"></a></th>
        <td align="center"><img src="BoxingNoFrameskip-v4_paad.gif" alt="Pong-attack-natural.gif" style="width:60%;"></a></th>
        </tr>
        <tr>
        <td align="center"><strong>Pong</strong>, <em>DQN</em> <br> clean reward: <strong>21</strong> <br> (agent: right paddle)</td>
        <td align="center"><strong>Pong</strong>, <em>DQN</em> <br> reward under strongest <br> previous attack: <strong>-14</strong> <br></td>
        <td align="center"><strong>Pong</strong>, <em>DQN</em> <br> reward under our <br> PA-AD attack: <strong>-21</strong> <br></td>
        <td align="center"><strong>Boxing</strong>, <em>DQN</em> <br> clean reward: <strong>94</strong> <br>(agent: left boxer)</td>
        <td align="center"><strong>Boxing</strong>, <em>DQN</em> <br> reward under strongest <br> previous attack: <strong>69</strong> <br></td>
        <td align="center"><strong>Boxing</strong>, <em>DQN</em> <br> reward under our <br> PA-AD attack: <strong>8</strong> <br></td>
        </tr>
        </tbody>
    </table>
    </center>
      </div>
    </div>
  </div><br><br><br>





  <!-- Abstract -->
  <div class="container">
    <h4 id="Abstract" style="padding-top: 70px; margin-top: -80px; ">Abstract</h4><hr>
    <div style="text-align: justify">
      Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within
      some constraints) is crucial for understanding the robustness of RL agents. However, finding the optimal adversary is challenging, in terms of both whether we
      can find the optimal attack and how efficiently we can find it. Existing works on
      adversarial RL either use heuristics-based methods that may not find the strongest
      adversary, or directly train an RL-based adversary by treating the agent as a part of
      the environment, which can find the optimal adversary but may become intractable
      in a large state space. This paper introduces a novel attacking method to find the
      optimal attacks through collaboration between a designed function named “actor”
      and an RL-based learner named “director”. The actor crafts state perturbations for
      a given policy perturbation direction, and the director learns to propose the best
      policy perturbation directions. Our proposed algorithm, PA-AD, is theoretically
      optimal and significantly more efficient than prior RL-based works in environments with large state spaces. Empirical results show that our proposed PA-AD
      universally outperforms state-of-the-art attacking methods in various Atari and
      MuJoCo environments. By applying PA-AD to adversarial training, we achieve
      state-of-the-art empirical robustness in multiple tasks under strong adversaries.
    </div>
  </div><br><br>

  <!-- Paper -->
  <!-- <div class="container">
    <h4 id="Paper" style="padding-top: 70px; margin-top: -80px;">Paper</h4><hr>

    <div class="row">
      <div class="col-md-12">
        <b>  Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL</b><br>

        Yanchao Sun </a>,
        Ruijie Zheng </a>,
        Yongyuan Liang </a>,
        Furong Huang </a><br>

        <i><font color="black"> </font></i><br>
        <a target="_blank" href="https://arxiv.org/abs/2106.05087">[Paper]</a>&nbsp;
        <a target="_blank" href="">[Code]</a>&nbsp;
        <a target="_blank" href="">[Talk]</a>&nbsp;
        <a target="_blank" href="">[Slides]</a>&nbsp;

        Modal
        <div class="modal fade" id="bibtex" tabindex="-1" role="dialog" aria-labelledby="exampleModalCenterTitle" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered modal-lg" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="bibtex-title">BibTeX</h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div><br><br> --> 

  <!-- Method: Breifly Describe our algorithm -->
  <div class="container">
    <h4 id="Method" style="padding-top: 70px; margin-top: -80px; ">Method</h4><hr>
    <b>Previous Appoach</b><br>
    <ul>
      Prior works in evasion RL generally falls into two categories: <b>Heuristic Attack</b> and <b>RL-based Attack</b><br>
      <li><b>Heuristic Attack</b>: at every step set an attacking objective by heuristic. For example, use fast gradient sign method (<b>FGSM</b>) to find
      the perturbation that could cause the agent to select the action corresponding to the minimum current Q value (<b>MaxWorst</b>). Or at every step, use FGSM to stop the agents
      from choosing the best action (<b>MinBest</b>). Or at every step, find a perturbation that cause the action distribution to have the largest KL divergence from the original
      distribution (<b>MaxDiff</b>). Heuristic Attack is often fast to compute and works decently well in practice, but it has no optimality guarantee.</li>
      <li><b>RL-based attack</b>: <a href=https://arxiv.org/pdf/2101.08452.pdf>Zhang et al.</a> establish the theory that learning optimal attacker in RL corresponds to learning
      an optimal policy in an adversary-induced MDP. Based on this insight, they use an end-to-end RL formulation to learn the optimal state adversary policy. However, because
      both the state and action space of the attacker's policy have the same size as the state space of the original environment, it becomes intractable when the original environments
      has a large state space, such as image space in Atari Games.  </li>
  </ul>
    <b> Our Approach (PA-AD) </b>:
      Instead of learning an end-to-end adversary policy that maps observations directly into observation perturbation, we decouple the whole attacking process into two
      simpler components: policy perturbation and state perturbation, solved by a <b>“director”</b> and an <b>“actor”</b> respectively. The director learns the
      optimal policy perturbing direction with RL methods, while the actor crafts adversarial states at every step such that the victim policy is perturbed towards the given direction.
      In this way, we obtain a both optimal and efficient attacking algorithm which is extremely powerful on environments with large observation space.
      <img class="img-responsive img-rounded" src="demo.PNG" style="width:100%; height:100%" alt="">

    <div style="text-align: justify">

    </div>
  </div><br><br>

  <!-- Experiments: show our experimental results -->
  <div class="container">
    <h4 id="Results" style="padding-top: 70px; margin-top: -80px; ">Results</h4><hr>
    <div style="text-align: justify">
    Empirically, we verify that our PA-AD algorithm almost always outperforms all existing attacking methods by a large margin on various OpenAI Gym environments, including Atari and MuJoCo tasks.
    Here is a comparison of PA-AD against existing attacking methods on seven Atari Games and four MuJoco tasks.
    We compute the average episode rewards plus/minus standard deviation of vanilla DQN and A2C or PPO agents under different evasion
attack methods
      <center><img class="img-responsive img-rounded" src="./atari.PNG" style="width:80%; height:80%" alt=""></center>
      <!--<center><figcaption><h1 style="font-size:1vw"> Comparison of PA-AD against existing attacking methods on Atari Games:<br> Average episode rewards ± standard deviation of vanilla DQN and A2C agents<br> under different evasion
attack methods </figcaption><br></center>-->
    Here, we have the comparison of PA-AD attacks against other existing
    attacks with different choices of epsilon on three Atari Games: Boxing, Pong, and RoadRunner respectively.
      <center><img class="img-responsive img-rounded" src="mujoco.PNG" style="width:80%; height:80%" alt=""></center><br>
  <!--<center><figcaption><h1 style="font-size:1vw"> Comparison of PA-AD against existing attacking methods on Mujoco Environments:<br> Average episode rewards ± standard deviation of vanilla PPO agents<br> under different evasion
attack methods </figcaption><br></center>-->
In addition to the given epsilon shown in the table above, we also have the comparison of PA-AD attacks against other attackers under different choices of
epsilon on three Atari Games: Boxing, Pong, RoadRunner respectively.<br>
<div class="container" style="padding-top: 10px; font-size: 20px">
  <div align="center">
      <div class="center">
  <img class="img-responsive img-rounded" src="dqn_boxing_eps.PNG" style="width:25%; height:25%" alt="">&nbsp;&nbsp;
  <img class="img-responsive img-rounded" src="dqn_pong_eps.PNG" style="width:25%; height:25%" alt="">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <img class="img-responsive img-rounded" src="dqn_roadrunner_eps.PNG" style="width:25%; height:25%" alt="">&nbsp;&nbsp;
</div>
</div></div><br>
Besides, when combining with existing robust training methods, our algorithm PA-AD could significantly
improve the robustness of the trained RL policies.
    <center><img class="img-responsive img-rounded" src="mujoco_atla.PNG" style="width:80%; height:80%" alt=""></center><br>
    </div>
  </div><br><br>

  
  <!-- <div class="container">
    <h4 id="Citation" style="padding-top: 70px; margin-top: -80px;">Citation</h4><hr>
    <div class="row">
      <div class="col-md-12">
        <b><a href="https://arxiv.org/pdf/2003.08938.pdf">Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations</a></b><br/>
          Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, Cho-Jui Hsieh1 <br/>
        <i>Neural Information Processing Systems (Neurips 2020)</i><br/>
        <b><a href="https://arxiv.org/abs/2101.08452">Robust Reinforcement Learning on State Observations with Learned Optimal Adversary</a></b><br/>
        Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh1 <br/>
        <i> International Conference on Learning Representations (ICLR 2021)</i><br/>
        <b><a href="https://arxiv.org/abs/2101.08452">Robust Reinforcement Learning on State Observations with Learned Optimal Adversary</a></b><br/>
        Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh1 <br/>
        <i> International Conference on Learning Representations (ICLR 2021)</i><br/>
      </div>
    </div>
  </div> <br> -->

  <div class="container">
    <h4 id="Cite" style="padding-top: 70px; margin-top: -80px;">Cite This Paper</h4><hr>
    <div class="row">
      <div class="col-md-12">
        @inproceedings{<br>
          &nbsp;&nbsp;&nbsp;&nbsp;sun2022who,<br>
          &nbsp;&nbsp;&nbsp;&nbsp;title={Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep {RL}},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;author={Yanchao Sun and Ruijie Zheng and Yongyuan Liang and Furong Huang},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;booktitle={International Conference on Learning Representations},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;year={2022},<br>
          &nbsp;&nbsp;&nbsp;&nbsp;url={https://openreview.net/forum?id=JM2kFbJvvI}<br>
          }<br>
      </div>
    </div>
  </div> <br>

  <!-- Footer -->
  <div class="container col-md-9">
    <hr>
    <center>
      <footer>
        <p>© University of Maryland College Park 2022</p>
      </footer>
    </center>
  </div>


  <!-- Bootstrap core JavaScript -->
  <!-- Placed at the end of the document so the pages load faster -->
  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>

</body>

</html>
